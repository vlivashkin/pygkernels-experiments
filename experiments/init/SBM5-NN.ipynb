{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from networkx.algorithms.approximation import clique\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.svm import SVC, SVR, LinearSVC, LinearSVR\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils import SBM_Data, Datasets_Data, load_or_calc_and_save, ytrue_to_partition, calc_avranks, RFE, RFE_LOO, OneVsRest_custom, OneHotEncoding_custom\n",
    "\n",
    "sys.path.append('../../pygkernels')\n",
    "from pygkernels.scenario import d3_category20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prepare columns: 100%|██████████| 87/87 [00:00<00:00, 651.25it/s]\n"
     ]
    }
   ],
   "source": [
    "data_hub = SBM_Data()\n",
    "_, results_modularity_any3, modularity_results = data_hub.load_precalculated()\n",
    "\n",
    "X_3d, ari_3d = data_hub.make_dataset(return_clf=False)\n",
    "# X_train_3d,    ari_train_3d    = X_3d[:, :60], ari_3d[:, :60]\n",
    "X_trainval_3d, ari_trainval_3d = X_3d[:, :80], ari_3d[:, :80]\n",
    "# X_val_3d,      ari_val_3d      = X_3d[:, 60:80], ari_3d[:, 60:80]\n",
    "X_test_3d,     ari_test_3d     = X_3d[:, 80:], ari_3d[:, 80:]\n",
    "\n",
    "X,          ari          = X_3d.reshape(-1, X_3d.shape[2]),          ari_3d.reshape(-1, ari_3d.shape[2])\n",
    "# X_train,    ari_train    = X_train_3d.reshape(-1, X_3d.shape[2]),    ari_train_3d.reshape(-1, ari_3d.shape[2])\n",
    "X_trainval, ari_trainval = X_trainval_3d.reshape(-1, X_3d.shape[2]), ari_trainval_3d.reshape(-1, ari_3d.shape[2])\n",
    "# X_val,      ari_val      = X_val_3d.reshape(-1, X_3d.shape[2]),      ari_val_3d.reshape(-1, ari_3d.shape[2])\n",
    "X_test,     ari_test     = X_test_3d.reshape(-1, X_3d.shape[2]),     ari_test_3d.reshape(-1, ari_3d.shape[2])\n",
    "\n",
    "feature_names = data_hub.allowed_features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_hub.kernel_names.index('SCT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 1: the best measure for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 1. best: logHeat (7), trainval: 0.682, test: 0.680\n"
     ]
    }
   ],
   "source": [
    "baseline1_kernel_idx = np.argmax(np.mean(ari_trainval, axis=0))\n",
    "baseline1_kernel_name = data_hub.kernel_names[baseline1_kernel_idx]\n",
    "baseline1_trainval_ari = np.mean(ari_trainval[:, baseline1_kernel_idx])\n",
    "baseline1_test_ari = np.mean(ari_test[:, baseline1_kernel_idx])\n",
    "print(f'baseline 1. best: {baseline1_kernel_name} ({baseline1_kernel_idx}), '\n",
    "      f'trainval: {baseline1_trainval_ari:.3f}, test: {baseline1_test_ari:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 2: best measure for every column (based on train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline 2. trainval: 0.698, test: 0.698\n"
     ]
    }
   ],
   "source": [
    "baseline2_kernel_indices = np.argmax(np.mean(ari_trainval_3d, axis=1), axis=1)\n",
    "baseline2_trainval_ari = np.mean(np.mean(ari_trainval_3d, axis=1)[range(len(baseline2_kernel_indices)), baseline2_kernel_indices])\n",
    "baseline2_test_ari = np.mean(np.mean(ari_test_3d, axis=1)[range(len(baseline2_kernel_indices)), baseline2_kernel_indices])\n",
    "print(f'baseline 2. trainval: {baseline2_trainval_ari:.3f}, test: {baseline2_test_ari:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper bound 1: best measure for every column (based on val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound 1. trainval: 0.698, test: 0.704\n"
     ]
    }
   ],
   "source": [
    "upperbound1_kernel_indices = np.argmax(np.mean(ari_trainval_3d, axis=1), axis=1)\n",
    "upperbound1_trainval_ari = np.mean(np.mean(ari_trainval_3d, axis=1)[range(len(upperbound1_kernel_indices)), upperbound1_kernel_indices])\n",
    "upperbound1_kernel_indices = np.argmax(np.mean(ari_test_3d, axis=1), axis=1)\n",
    "upperbound1_test_ari = np.mean(np.mean(ari_test_3d, axis=1)[range(len(upperbound1_kernel_indices)), upperbound1_kernel_indices])\n",
    "print(f'upper bound 1. trainval: {upperbound1_trainval_ari:.3f}, test: {upperbound1_test_ari:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upper bound 2: best measure for every graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper bound 2. trainval: 0.736, test: 0.738\n"
     ]
    }
   ],
   "source": [
    "upperbound2_trainval_ari = np.mean(np.max(ari_trainval, axis=1))\n",
    "upperbound2_test_ari = np.mean(np.max(ari_test, axis=1))\n",
    "print(f'upper bound 2. trainval: {upperbound2_trainval_ari:.3f}, test: {upperbound2_test_ari:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ours NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBMDataset(Dataset):\n",
    "    def __init__(self, part='train'):\n",
    "        super().__init__()\n",
    "        data_hub = SBM_Data()\n",
    "        \n",
    "#         chosen_feature_names = ['n', 'k', 'p_in', 'p_out']\n",
    "#         chosen_feature_names = ['log(n)/k * p_in/p_out', 'avg_sp']\n",
    "#         chosen_feature_names = ['sbm_neighbour_score', 'modularity']\n",
    "#         chosen_feature_names = ['log(n)/k * p_in/p_out', 'median_deg', 'max_clique/(n/k)']\n",
    "        chosen_feature_names = ['sbm_neighbour_score', 'modularity', 'avg_sp', 'std_sp']\n",
    "\n",
    "        chosen_features = []\n",
    "        for chosen_feature in chosen_feature_names:\n",
    "            chosen_features.append(data_hub.allowed_features_list.index(chosen_feature))\n",
    "        chosen_features\n",
    "        \n",
    "        X_3d, ari_3d = data_hub.make_dataset(return_clf=False)\n",
    "        X_3d = X_3d[:, :, chosen_features]\n",
    "        \n",
    "        if part == 'train':\n",
    "            X, y = X_3d[:, :60].reshape(-1, X_3d.shape[2]), ari_3d[:, :60].reshape(-1, ari_3d.shape[2])\n",
    "        elif part == 'val':\n",
    "            X, y = X_3d[:, 60:80].reshape(-1, X_3d.shape[2]), ari_3d[:, 60:80].reshape(-1, ari_3d.shape[2])\n",
    "        elif part == 'test':\n",
    "            X, y = X_3d[:, 80:].reshape(-1, X_3d.shape[2]), ari_3d[:, 80:].reshape(-1, ari_3d.shape[2])\n",
    "        assert X.shape[0] == y.shape[0]\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "        print(f'{part}: {self.X.shape[0]}')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        Xi, yi = self.X[index], self.y[index]\n",
    "        return Xi, yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels):\n",
    "        super(PolicyNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_channels, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "        self.fc3 = nn.Linear(256, 512)\n",
    "        self.fc4 = nn.Linear(512, 256)\n",
    "        self.fc5 = nn.Linear(256, 128)\n",
    "        self.fc6 = nn.Linear(128, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return torch.softmax(x, axis=1)\n",
    "\n",
    "class RMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss,self).__init__()\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "\n",
    "    def forward(self,x,y):\n",
    "        loss = torch.sqrt(self.criterion(x, y))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prepare columns: 100%|██████████| 87/87 [00:00<00:00, 692.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 5220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prepare columns: 100%|██████████| 87/87 [00:00<00:00, 438.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val: 1740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "prepare columns: 100%|██████████| 87/87 [00:00<00:00, 688.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 1740\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "n_epoch = 2000\n",
    "batch_size = 50\n",
    "\n",
    "train_dataset = SBMDataset(part='train')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataset = SBMDataset(part='val')\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10)\n",
    "test_dataset = SBMDataset(part='test')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "sampleX, sampley = val_dataset[0]\n",
    "\n",
    "model = PolicyNet(sampleX.shape[0], sampley.shape[0]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0012)\n",
    "scheduler = StepLR(optimizer, step_size=170, gamma=0.5)\n",
    "criterion = RMSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3eefb6174764b76be10b69224dff5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best: 0, loss=0.080, ari=0.674\n",
      "new best: 1, loss=0.071, ari=0.683\n",
      "new best: 3, loss=0.064, ari=0.690\n",
      "new best: 8, loss=0.064, ari=0.690\n",
      "new best: 9, loss=0.064, ari=0.691\n",
      "new best: 14, loss=0.063, ari=0.691\n",
      "new best: 18, loss=0.062, ari=0.692\n",
      "new best: 43, loss=0.062, ari=0.693\n",
      "new best: 60, loss=0.061, ari=0.693\n",
      "new best: 80, loss=0.061, ari=0.693\n",
      "new best: 166, loss=0.061, ari=0.692\n",
      "new best: 176, loss=0.060, ari=0.693\n",
      "new best: 186, loss=0.060, ari=0.693\n",
      "new best: 198, loss=0.060, ari=0.693\n",
      "new best: 202, loss=0.060, ari=0.693\n",
      "new best: 326, loss=0.059, ari=0.693\n",
      "new best: 348, loss=0.059, ari=0.694\n",
      "new best: 359, loss=0.059, ari=0.694\n",
      "new best: 365, loss=0.059, ari=0.694\n",
      "new best: 369, loss=0.059, ari=0.694\n",
      "new best: 404, loss=0.058, ari=0.694\n",
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "best_val_loss = 1000\n",
    "for epoch in tqdm(list(range(n_epoch))):\n",
    "    writer.add_scalar('train/lr', scheduler.get_last_lr()[0], epoch)\n",
    "                  \n",
    "    for idx, (X, ari) in enumerate(train_dataloader):\n",
    "        X, ari = X.to(device), ari.to(device)\n",
    "        y_true = torch.max(ari, dim=1, keepdims=True)[0]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        y_pred = torch.sum(ari * output, dim=1, keepdims=True)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ari = torch.mean(y_pred.detach())\n",
    "        writer.add_scalar('train/loss', loss.item(), epoch * len(train_dataset) + idx * batch_size)\n",
    "        writer.add_scalar('train/ari', ari.item(), epoch * len(train_dataset) + idx * batch_size)\n",
    "    \n",
    "    val_loss, val_ari, counter = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, ari in val_dataloader:\n",
    "            X, ari = X.to(device), ari.to(device)\n",
    "            y_true = torch.max(ari, dim=1, keepdims=True)[0]\n",
    "            output = model(X)\n",
    "            y_pred = ari[range(len(output)), torch.max(output, dim=1)[1]].unsqueeze(1)\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            val_loss += loss.item()\n",
    "            val_ari += torch.mean(y_pred.detach())\n",
    "            counter += 1\n",
    "    val_loss = val_loss / counter\n",
    "    val_ari = val_ari / counter\n",
    "    writer.add_scalar('val/loss', val_loss, epoch)\n",
    "    writer.add_scalar('val/ari', val_ari, epoch)\n",
    "    writer.flush()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if best_val_loss > val_loss:\n",
    "        print(f'new best: {epoch}, loss={val_loss:.3f}, ari={val_ari:.3f}')\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PolicyNet(\n",
       "  (fc1): Linear(in_features=4, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc6): Linear(in_features=128, out_features=25, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PolicyNet(sampleX.shape[0], sampley.shape[0]).to(device)\n",
    "model.load_state_dict(torch.load('model_best.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 0.059, ari: 0.696\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_ari, counter = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for X, ari in test_dataloader:\n",
    "        X, ari = X.to(device), ari.to(device)\n",
    "        y_true = torch.max(ari, dim=1, keepdims=True)[0]\n",
    "        output = model(X)\n",
    "        y_pred = ari[range(len(output)), torch.max(output, dim=1)[1]].unsqueeze(1)\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        test_loss += loss.item()\n",
    "        test_ari += torch.mean(y_pred.detach())\n",
    "        counter += 1\n",
    "test_loss = test_loss / counter\n",
    "test_ari = test_ari / counter\n",
    "print(f'test loss: {test_loss:.3f}, ari: {test_ari:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
