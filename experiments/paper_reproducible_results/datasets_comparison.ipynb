{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import powerlaw\n",
    "from collections import defaultdict, Counter\n",
    "from networkx.algorithms.community.quality import modularity as nx_modularity\n",
    "\n",
    "\n",
    "sys.path.append('../../pygkernels')\n",
    "from pygkernels.data import Datasets\n",
    "from pygkernels.measure import kernels, Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2nx(A: np.ndarray, partition: np.ndarray):\n",
    "    G = nx.from_numpy_matrix(A)\n",
    "    nx.set_node_attributes(G, dict(enumerate(partition)), 'community')\n",
    "    return G\n",
    "\n",
    "def partition2communities(partition):\n",
    "    result = defaultdict(list)\n",
    "    for idx, class_ in enumerate(partition):\n",
    "        result[class_].append(idx)\n",
    "    return list(result.values())\n",
    "\n",
    "def power_law(values, maxval=200):\n",
    "    tau = powerlaw.Fit(values, verbose=False).alpha\n",
    "    if tau > maxval or np.isnan(tau):\n",
    "        tau = maxval\n",
    "    return tau\n",
    "\n",
    "feature_inv = lambda x, power: (1 - (1 / x**power))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_ROOT = '/data/phd/pygkernels/cache/kkmeans_init_datasets'\n",
    "dataset_names = [\n",
    "    'dolphins',\n",
    "    'football',\n",
    "    'karate',\n",
    "    'news_2cl1_0.1', 'news_2cl2_0.1', 'news_2cl3_0.1',\n",
    "    'news_3cl1_0.1', 'news_3cl2_0.1', 'news_3cl3_0.1',\n",
    "    'news_5cl1_0.1', 'news_5cl2_0.1', 'news_5cl3_0.1',\n",
    "    'polblogs',\n",
    "    'polbooks',\n",
    "    'sp_school_day_1', 'sp_school_day_2',\n",
    "    'cora_DB', 'cora_EC', 'cora_HA', 'cora_HCI', 'cora_IR', 'cora_Net',\n",
    "    'eu-core',\n",
    "    'eurosis'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_norms = {\n",
    "    'tau1|sqrtinv': {'min': 0.27, 'max': 0.94},\n",
    "    'avg_degree|log': {'min': 0.86, 'max': 7.07},\n",
    "    'modularity': {'min': -0.46, 'max': 0.84}\n",
    "}\n",
    "\n",
    "for param, norms in param_norms.items():\n",
    "    param_norms[param]['width'] = norms['max'] - norms['min']\n",
    "    \n",
    "def normalize(param, param_name):\n",
    "    return (param - param_norms[param_name]['min']) / param_norms[param_name]['width']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сравнение результатов на датасетах\n",
    "\n",
    "Шаги:\n",
    "* Формируем список всех датасетов и мер, а также их параметров: tau1, avg degree, modularity\n",
    "* Открываем всё насчитанное\n",
    "* Находим для каждого датасета лучший скор каждой меры\n",
    "* Стратегия 1: выбираем для каждого датасета лучшую меру (upper bound)\n",
    "* Стратегия 2: выбираем для всех датасетов общую лучшую меру (SCT или SCCT)\n",
    "* Стратегия 3: выбираем для каждого датасета меру согласно его параметрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d82cb0cee69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filtered_dataset.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlfr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlfr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m idx_several_answers = [idx for idx, d in enumerate(lfr) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../paper_reproducible_results/lfr_result_grid.pkl', 'rb') as f:\n",
    "    grid = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_holder = Datasets()\n",
    "for dataset_name in dataset_names:\n",
    "    (A, partition), info = datasets_holder[dataset_name]\n",
    "    G = np2nx(A, partition)\n",
    "    \n",
    "    # Parameters\n",
    "    avg_degree = 2 * G.number_of_edges() / G.number_of_nodes()\n",
    "    node_degrees = [d for n, d in G.degree()]\n",
    "    tau1 = power_law(node_degrees, 100)\n",
    "    community_sizes = list(Counter(partition).values())\n",
    "    modularity = nx_modularity(G, partition2communities(partition))\n",
    "    data[dataset_name]['params'] = {\n",
    "        'tau1': tau1, \n",
    "        'tau1|sqrtinv': feature_inv(tau1, 0.5),\n",
    "        'avg_degree': avg_degree,\n",
    "        'avg_degree|log': np.log(avg_degree),\n",
    "        'modularity': modularity\n",
    "    }\n",
    "    \n",
    "    # Best measure according LFR\n",
    "    closest_dist, closest_recomendation = 1000, None\n",
    "    for out_x, out_y in zip(grid['out_x'], grid['out_y']):\n",
    "        diff1 = np.abs(out_x[0] - normalize(data[dataset_name]['params']['tau1|sqrtinv'], 'tau1|sqrtinv'))\n",
    "        diff2 = np.abs(out_x[1] - normalize(data[dataset_name]['params']['avg_degree|log'], 'avg_degree|log'))\n",
    "        diff3 = np.abs(out_x[2] - normalize(data[dataset_name]['params']['modularity'], 'modularity'))\n",
    "        dist = np.sqrt(diff1*diff1 + diff2*diff2 + diff3*diff3)\n",
    "        if dist < closest_dist:\n",
    "            closest_dist, closest_recomendation = dist, out_y\n",
    "    data[dataset_name]['lfr_recomendation'] = {\n",
    "        'closest_dist': closest_dist,\n",
    "        'closest_recomendation': closest_recomendation\n",
    "    }\n",
    "    \n",
    "    # Results\n",
    "    data[dataset_name]['results'] = {}\n",
    "    best_dataset_kernel_name, best_dataset_kernel_ari = None, -1\n",
    "    best_dataset_kernel_name_top6, best_dataset_kernel_ari_top6 = None, -1\n",
    "    for kernel in kernels:\n",
    "        dataset_kernel_results_mean = defaultdict(list)\n",
    "        for i in range(5):\n",
    "            with open(f'{CACHE_ROOT}/by_column_and_kernel/{dataset_name}_{kernel.name}_results_0{i}.pkl', 'rb') as f:\n",
    "                dataset_kernel_results = pickle.load(f)\n",
    "            for param, param_results in dataset_kernel_results.items():\n",
    "                if len(param_results) > 0:\n",
    "                    best_init = sorted(param_results, key=lambda x: -x['modularity'])[0]\n",
    "                    dataset_kernel_results_mean[param].append(best_init['score_ari'])\n",
    "                else:\n",
    "                    dataset_kernel_results_mean[param].append(0)\n",
    "        for k, v in dataset_kernel_results_mean.items():\n",
    "            dataset_kernel_results_mean[k] = np.mean(v)\n",
    "        best_ari = np.max(list(dataset_kernel_results_mean.values()))\n",
    "        data[dataset_name]['results'][kernel.name] = best_ari\n",
    "        \n",
    "        if best_ari > best_dataset_kernel_ari:\n",
    "            best_dataset_kernel_name, best_dataset_kernel_ari = kernel.name, best_ari\n",
    "        elif best_ari == best_dataset_kernel_ari:\n",
    "            if type(best_dataset_kernel_name) == list:\n",
    "                best_dataset_kernel_name.append(kernel.name)\n",
    "            else:\n",
    "                best_dataset_kernel_name = [best_dataset_kernel_name, kernel.name]\n",
    "                \n",
    "        if kernel.name in ['SCCT', 'logComm', 'logDF', 'RSP', 'Comm', 'NHeat']:\n",
    "            if best_ari > best_dataset_kernel_ari_top6:\n",
    "                best_dataset_kernel_name_top6, best_dataset_kernel_ari_top6 = kernel.name, best_ari\n",
    "            elif best_ari == best_dataset_kernel_ari_top6:\n",
    "                if type(best_dataset_kernel_name_top6) == list:\n",
    "                    best_dataset_kernel_name_top6.append(kernel.name)\n",
    "                else:\n",
    "                    best_dataset_kernel_name_top6 = [best_dataset_kernel_name_top6, kernel.name]\n",
    "\n",
    "    data[dataset_name]['best_measure'] = {\n",
    "        'name': best_dataset_kernel_name, \n",
    "        'ari': best_dataset_kernel_ari\n",
    "    }\n",
    "    \n",
    "    data[dataset_name]['best_measure_top6'] = {\n",
    "        'name': best_dataset_kernel_name_top6, \n",
    "        'ari': best_dataset_kernel_ari_top6\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dolphins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_measure = defaultdict(list)\n",
    "for dataset_name, results in data.items():\n",
    "    for kernel_name, ari in results['results'].items():\n",
    "        group_by_measure[kernel_name].append(ari)\n",
    "\n",
    "mean_by_measure = {k: np.mean(v) for k, v in group_by_measure.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(mean_by_measure.items()), columns=['measure', 'ari'])\n",
    "df.sort_values('ari', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_table = []\n",
    "for dataset_name, results in data.items():\n",
    "    strategies_table.append({\n",
    "        'dataset_name': dataset_name,\n",
    "        'best_measure name': 'SCCT',\n",
    "        'best_measure ari': results['results']['SCCT'],\n",
    "        'lfr_recomendation name': results['lfr_recomendation']['closest_recomendation'],\n",
    "        'lfr_recomendation ari': results['results'][results['lfr_recomendation']['closest_recomendation']],\n",
    "        'upper_bound_top6 name': results['best_measure_top6']['name'],\n",
    "        'upper_bound_top6 ari': results['best_measure_top6']['ari'],\n",
    "        'upper_bound name': results['best_measure']['name'],\n",
    "        'upper_bound ari': results['best_measure']['ari'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(strategies_table).to_excel('strategies_table.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
